# 两个文件中的共同数据


方案1：  

可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G  

所以不可能将其完全加载到内存中处理。  

考虑采取分而治之的方法。  



遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,...,a999）中。  

这样每个小文件的大约为300M。  



遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,...,b999）。  

这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,...,a999vsb999）中，不对应的小文件不可能有相同的url。  

然后我们只要求出1000对小文件中相同的url即可。  

求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash\_set中。  

然后遍历另一个小文件的每个url，看其是否在刚才构建的hash\_set中，如果是，那么就是共同的url，存到文件里面就可以了。  


方案2：  

如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。  
将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）.  

